language: scala

sudo: required

dist: trusty

cache:
  directories:
    - $HOME/.ivy2
    - $HOME/spark
    - $HOME/.cache/pip
    - $HOME/.pip-cache
    - $HOME/.sbt/launchers

jdk:
  - oraclejdk8

scala:
   - 2.11.8

sudo: false

addons:
  apt:
    packages:
      - axel

before_install:
  - export PATH=$HOME/.local/bin:$PATH
  - pip install --user codecov coverage

install:
  # Download spark 2.3.1
  - "[ -f spark ] || mkdir spark && cd spark && axel http://www-us.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz && cd .."
  - tar -xf ./spark/spark-2.3.1-bin-hadoop2.7.tgz
  - export SPARK_HOME=`pwd`/spark-2.3.1-bin-hadoop2.7
  - echo "spark.yarn.jars=$SPARK_HOME/jars/*.jar" > $SPARK_HOME/conf/spark-defaults.conf

  # Install Python deps.
  # The conda installation steps here are based on http://conda.pydata.org/docs/travis.html
  - wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O miniconda.sh
  - bash miniconda.sh -b -p $HOME/miniconda
  - export PATH="$HOME/miniconda/bin:$PATH"
  - hash -r
  - conda config --set always_yes yes --set changeps1 no
  - conda update -q conda
  # In case you need to debug any issues with conda
  - conda info -a
  # Install deps -- TODO: use the requirements.txt file.
  - deps='pip coverage py4j numpy scipy'

  # Test with python 3.6
  - conda create -p $HOME/py --yes $deps "python=3.6"
  - export PATH=$HOME/py/bin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
  - export PYTHONPATH="$SPARK_HOME/python:$PYTHONPATH"
  - pip install --upgrade codecov

script:
  # TODO: replace this with pip install once the setup.py is done.
  - export S3DHOME="/home/travis/build/astrolabsoftware/spark3D"
  - export PYTHONPATH="$S3DHOME:$S3DHOME/pyspark3d:$PYTHONPATH"
  # Test with Scala 2.11
  - export SCALA_BINARY_VERSION="2.11.8"

  # Scala tests
  - ./test_scala.sh $SCALA_BINARY_VERSION
  - bash <(curl -s https://codecov.io/bash) -cF scala

  # Python tests
  - ./test_python.sh $SCALA_BINARY_VERSION
  - bash <(curl -s https://codecov.io/bash) -cF python
